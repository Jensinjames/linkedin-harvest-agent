## Product Requirements Document (PRD)

**Application:** LinkedAgent
**Version:** 1.0
**Date:** October 26, 2023
**Author:** AI-Powered Serverless Product Manager

---

### 1. Executive Summary

**1.1. Product Vision & Value Proposition**

LinkedAgent is a SaaS platform that empowers recruiters, growth marketers, and data analysts to efficiently and reliably scrape and enrich LinkedIn profiles at scale. It addresses the critical need for high-volume, automated LinkedIn data extraction, overcoming the limitations of manual processes and unreliable scraping tools. Our value proposition lies in providing a robust, transparent, and user-friendly platform that delivers high-quality data, minimizes scraping errors, and maximizes user productivity. LinkedAgent is built on a modern, serverless architecture using Next.js, Convex, and Vercel, ensuring scalability, performance, and ease of management.

**1.2. Target User Segments & Use Cases**

*   **Recruiters:** Bulk data enrichment of candidate profiles for talent acquisition.
*   **Sales & Growth Teams:** Targeting prospects with enriched lead information.
*   **Data Analysts:** Scraping public LinkedIn profiles for research and analysis.
*   **Agencies:** Providing lead generation and data enrichment services to clients.
*   **Technical Founders:** Automating talent pipeline management and research.

**Use Cases:**

*   Uploading a list of LinkedIn profile URLs and receiving enriched data (name, title, company, location) in an Excel file.
*   Configuring scraping settings, including proxy rotation and retry logic.
*   Monitoring scraping progress in real-time.
*   Managing and organizing scraped data within the platform.
*   Downloading scraped data in various formats (e.g., CSV, Excel).

**1.3. Success Criteria & KPIs**

*   **Customer Acquisition Cost (CAC):** Track the cost to acquire each paying customer.
*   **Customer Lifetime Value (CLTV):** Project the total revenue generated by each customer over their relationship with LinkedAgent.
*   **Churn Rate:** Measure the percentage of customers who cancel their subscriptions over a given period.
*   **Monthly Recurring Revenue (MRR):** Monitor the predictable revenue generated each month.
*   **Active Users (DAU/MAU):** Number of Daily/Monthly Active Users.
*   **Conversion Rate:** Percentage of users who convert from free to paid subscriptions.
*   **Average Scraping Batch Size:** Average number of profiles scraped per batch.
*   **Scraping Success Rate:** Percentage of successfully scraped profiles.
*   **Average Profile Enrichment Time:** Time taken to enrich a profile.
*   **User Satisfaction (NPS/CSAT):** Collect user feedback to measure satisfaction.
*   **Uptime:** System uptime to ensure availability and reliability.
*   **Error Rate:** Number of errors during scraping and data processing.

---

### 2. User Experience Requirements

**2.1. Core User Flows**

*   **Registration & Login:**
    *   User registers with email and password.
    *   User receives a verification email.
    *   User logs in securely with email and password.
    *   Password reset functionality.
*   **Upload & Configure Scraping:**
    *   User uploads a CSV/Excel file containing LinkedIn profile URLs.
    *   User configures scraping settings (proxy rotation, retry attempts, data fields to extract).
    *   User selects output file format (Excel, CSV).
*   **Monitor Scraping Progress:**
    *   Real-time progress bar showing the status of the scraping job.
    *   Detailed logs of scraped profiles and any errors.
    *   Ability to pause, resume, or cancel the scraping job.
*   **Download & Manage Data:**
    *   User downloads the enriched data in the selected format.
    *   Data is organized by batch and date.
    *   User can view and delete previous scraping jobs.
*   **Account Management:**
    *   User can update their profile.
    *   User can view their subscription status.
    *   User can manage their billing information.

**2.2. Mobile-Responsive Design Requirements**

*   **Next.js Implementation:** Utilize Next.js's built-in features for responsive design.
*   **Responsive Layout:** Use CSS frameworks (e.g., Tailwind CSS, Material UI) to create responsive layouts that adapt to different screen sizes.
*   **Mobile Navigation:** Implement a responsive navigation menu (e.g., hamburger menu) on smaller screens.
*   **Touch-Friendly Interactions:** Ensure all UI elements are touch-friendly, with sufficient spacing and target sizes.
*   **Image Optimization:** Optimize images for different screen resolutions using `next/image`.

**2.3. Progressive Web App (PWA) Considerations**

*   **Service Worker:** Implement a service worker to enable offline capabilities (e.g., caching assets, offline access to job status).
*   **Web App Manifest:** Create a `manifest.json` file to define the app's metadata (name, icons, start URL, etc.).
*   **HTTPS:** Ensure the application is served over HTTPS for security and PWA compatibility.
*   **Install Prompt:** Provide a prompt for users to install the app on their devices.

---

### 3. Functional Requirements by Feature

**3.1. MCP-Powered Snapshot Scraping**

*   **User Story:** As a user, I want to be able to scrape LinkedIn profile data accurately and efficiently using a lightweight method, so I can avoid being blocked and get the most reliable data.

*   **Acceptance Criteria:**
    *   Scraping uses Playwright's accessibility tree to extract data.
    *   Data extraction includes name, title, company, location, and any other specified fields.
    *   The scraping process is resistant to LinkedIn's anti-scraping measures.
    *   The scraping speed is optimized for efficiency.

*   **Next.js Routing & Page Requirements:**
    *   No dedicated page. This is a backend process triggered by the user's file upload and configuration settings.
    *   The scraping process is initiated from a Convex function.
    *   Real-time status updates are displayed on the scraping progress page.

*   **Convex Schema & Function Specifications:**
    ```typescript
    // Convex Schema (convex/schema.ts)
    import { defineSchema, defineTable, } from "convex/server";
    import { v } from "convex/values";

    export default defineSchema({
      scrapingJobs: defineTable({
        userId: v.string(), // User ID from Convex Auth
        fileId: v.id("_storage"), // Convex storage file ID
        fileName: v.string(),
        status: v.union(
          v.literal("pending"),
          v.literal("running"),
          v.literal("completed"),
          v.literal("failed"),
          v.literal("paused"),
        ),
        progress: v.number(), // Percentage complete (0-100)
        totalProfiles: v.number(),
        profilesScraped: v.number(),
        settings: v.object({ // User-configurable scraping settings
          proxyRotation: v.boolean(),
          retryAttempts: v.number(),
          dataFields: v.array(v.string()),
        }),
        startTime: v.optional(v.number()),
        endTime: v.optional(v.number()),
        error: v.optional(v.string()),
      }).index("by_user_status", ["userId", "status"]),
    });
    ```

    ```typescript
    // Convex Functions (convex/scraping.ts)
    import { mutation, query } from "./_generated/server";
    import { api, internal } from "./_generated/api";
    import { Doc, Id } from "./_generated/dataModel";
    import { v } from "convex/values";
    import { getUser } from "./auth";

    export const startScraping = mutation({
      args: {
        fileId: v.id("_storage"),
        fileName: v.string(),
        settings: v.object({
          proxyRotation: v.boolean(),
          retryAttempts: v.number(),
          dataFields: v.array(v.string()),
        }),
        totalProfiles: v.number(),
      },
      handler: async (ctx, args) => {
        const user = await getUser(ctx);
        if (!user) {
          throw new Error("Unauthorized");
        }

        const jobId = await ctx.db.insert("scrapingJobs", {
          userId: user.subject,
          fileId: args.fileId,
          fileName: args.fileName,
          status: "pending",
          progress: 0,
          totalProfiles: args.totalProfiles,
          profilesScraped: 0,
          settings: args.settings,
        });

        await ctx.scheduler.runAfter(0, internal.scraping.processScrapingJob, {
          jobId,
        });

        return jobId;
      },
    });

    export const getScrapingJobs = query({
      args: {},
      handler: async (ctx) => {
        const user = await getUser(ctx);
        if (!user) {
          return [];
        }
        return await ctx.db
          .query("scrapingJobs")
          .withIndex("by_user_status", (q) =>
            q.eq("userId", user.subject).order("desc"),
          )
          .collect();
      },
    });
    ```

    ```typescript
    // Convex Internal Function (convex/scraping.ts)
    import { internalMutation } from "./_generated/server";
    import { api } from "./_generated/api";

    export const processScrapingJob = internalMutation({
      args: {
        jobId: v.id("scrapingJobs"),
      },
      handler: async (ctx, args) => {
        // 1. Retrieve Job Details
        const job = await ctx.db.get(args.jobId);
        if (!job) {
          console.error("Job not found:", args.jobId);
          return;
        }

        // 2. Update Status to Running
        await ctx.db.patch(args.jobId, {
          status: "running",
          startTime: Date.now(),
        });

        // 3. Fetch File from Convex Storage.
        const fileContent = await ctx.storage.get(job.fileId);

        if (!fileContent) {
          await ctx.db.patch(args.jobId, {
            status: "failed",
            error: "File not found in storage.",
          });
          return;
        }

        const urls = (fileContent as string).split(/\r?\n/).filter(Boolean);
        const totalProfiles = urls.length;

        // 4. Iterate through URLs and Scrape
        let profilesScraped = 0;
        let errorMessages: string[] = [];

        for (const url of urls) {
          try {
            // Perform Scraping using Playwright and Accessibility Tree
            const scrapedData = await scrapeLinkedInProfile(url, job.settings);

            // Save scraped data to Convex (or other storage)
            await ctx.db.insert("scrapedData", {
              jobId: args.jobId,
              profileUrl: url,
              data: scrapedData,
            });

            profilesScraped++;
            const progress = (profilesScraped / totalProfiles) * 100;
            await ctx.db.patch(args.jobId, {
              profilesScraped,
              progress,
            });
          } catch (error: any) {
            console.error(`Error scraping ${url}:`, error);
            errorMessages.push(
              `Error scraping ${url}: ${error.message || error}`,
            );
            // Implement retry logic here if needed (based on job.settings.retryAttempts)
          }
        }

        // 5. Update Job Status based on success/failure
        await ctx.db.patch(args.jobId, {
          status: profilesScraped === totalProfiles ? "completed" : "failed",
          endTime: Date.now(),
          error: errorMessages.length > 0 ? errorMessages.join("\n") : undefined,
          progress: 100, // Ensure 100% complete on completion
        });

        // 6. Trigger Excel Generation (optional)
        if (profilesScraped > 0) {
          ctx.scheduler.runAfter(
            0,
            internal.scraping.generateExcelReport,
            { jobId: args.jobId },
          );
        }
      },
    });
    ```

    ```typescript
    // Convex Internal Function (convex/scraping.ts)
    import { internalMutation } from "./_generated/server";
    import { api } from "./_generated/api";
    import { Id } from "./_generated/dataModel";

    export const generateExcelReport = internalMutation({
      args: {
        jobId: v.id("scrapingJobs"),
      },
      handler: async (ctx, args) => {
        // 1. Fetch Scraped Data for the Job
        const scrapedData = await ctx.db
          .query("scrapedData")
          .filter((q) => q.eq(q.field("jobId"), args.jobId))
          .collect();

        // 2. Convert Scraped Data to Excel
        const wb = xlsx.utils.book_new();
        const ws = xlsx.utils.json_to_sheet(
          scrapedData.map((item) => item.data),
        );
        xlsx.utils.book_append_sheet(wb, ws, "LinkedIn Data");
        const excelBuffer = xlsx.write(wb, {
          bookType: "xlsx",
          type: "buffer",
        });

        // 3. Store Excel file in Convex Storage
        const excelFileId = await ctx.storage.store(
          excelBuffer,
          `${args.jobId}.xlsx`,
        );

        // 4. Update Job with the Excel File ID
        await ctx.db.patch(args.jobId, {
          excelFileId,
        });
      },
    });
    ```

    ```typescript
    // Playwright helper function (in a separate file, e.g., utils/scraping.ts)
    import { chromium } from 'playwright';

    async function scrapeLinkedInProfile(url: string, settings: any) {
      const browser = await chromium.launch({ headless: true });
      const page = await browser.newPage();

      try {
        await page.goto(url, { waitUntil: 'networkidle' });

        // Extract data using accessibility tree
        const name = await page.locator('h1.text-heading-xlarge.inline.t-24.v-align-middle.break-words').textContent();
        const title = await page.locator('.text-body-medium.break-words').nth(0).textContent();
        const company = await page.locator('.text-body-medium.break-words').nth(1).textContent();
        const location = await page.locator('.text-body-small.inline.t-black--light.break-words').textContent();

        // Return scraped data
        return { name, title, company, location };

      } catch (error: any) {
        console.error(`Scraping error for ${url}:`, error);
        throw new Error(error.message || 'Scraping failed');
      } finally {
        await browser.close();
      }
    }
    ```

*   **Real-time Update Requirements:**
    *   Use Convex's `useQuery` hook to display real-time scraping progress.
    *   Update the progress bar and status messages in the UI.
    *   Implement a websocket connection for instant updates.

**3.2. Job Resumability**

*   **User Story:** As a user, I want to be able to resume a failed or paused scraping job, so I don't have to start over and waste time.

*   **Acceptance Criteria:**
    *   Jobs can be paused and resumed.
    *   Jobs can resume from the last successfully scraped URL.
    *   Retry logic handles temporary scraping failures.
    *   Partial results are preserved on job failure.

*   **Next.js Routing & Page Requirements:**
    *   A "Scraping Jobs" page to show the list of jobs and their statuses.
    *   A button to pause/resume the job on the "Scraping Jobs" page.
    *   A "Job Details" page to show detailed logs and progress.

*   **Convex Schema & Function Specifications:**
    *   See `scrapingJobs` table definition above.
    *   Modify the `processScrapingJob` function to:
        *   Check for job status before starting.
        *   Use the `profilesScraped` field to determine the starting point.
        *   Implement retry logic for individual URLs.

*   **Real-time Update Requirements:**
    *   Real-time updates on job status (paused, running, failed).
    *   Real-time progress updates.

**3.3. Proxy Smart Rotator**

*   **User Story:** As a user, I want the system to automatically rotate proxies to avoid being blocked by LinkedIn, so I can scrape data consistently.

*   **Acceptance Criteria:**
    *   Proxy rotation is enabled and configurable by the user.
    *   Proxies are dynamically selected based on success rate and LinkedIn blocking signals.
    *   The system detects and removes blocked proxies.
    *   The system provides a mechanism to add/manage proxies.

*   **Next.js Routing & Page